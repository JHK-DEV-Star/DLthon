{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c8528-dd0f-40e4-b531-3483ae0591a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy requests nlpaug transformers sacremoses nltk\n",
    "#!pip install hf_xet\n",
    "#!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411644ff-94a6-4cf8-a01c-a543e8973658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR, OneCycleLR\n",
    "from torchmetrics.classification import MulticlassF1Score, BinaryF1Score\n",
    "import sentencepiece as spm\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from typing import Callable, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405a9be-5a92-422e-9d6d-d6754b17f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"C:/Users/USER/Desktop/AIFFEL/work/aiffel_dl_thon_dktc_online_15\"\n",
    "path_to_chatbot = os.path.join(path_to_dataset, 'train_final.csv')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_train = pd.read_csv(path_to_chatbot)\n",
    "data_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29365a-aeee-4694-8eac-fe851b053839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train_SD_1000 = data_train.iloc[3950:].sample(1000).copy()\n",
    "#data_train_to_4950 = pd.concat((data_train.iloc[:3950], data_train_SD_1000), ignore_index=True).reset_index(drop=True)\n",
    "#data_train_to_4950[\"idx\"] = pd.Series([i for i in range(len(data_train))])\n",
    "#data_train_to_4950.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "#data_train_to_4950.sample(10)\n",
    "#print(len(data_train_to_4950))\n",
    "#data_train_to_4950.to_csv(os.path.join(path_to_dataset, 'train_final.csv'), index=False)\n",
    "#train_final=data_train_to_4950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a6da9-00f5-4276-af29-3cabde88324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"C:/Users/USER/Desktop/AIFFEL/work/aiffel_dl_thon_dktc_online_15\"\n",
    "path_to_chatbot = os.path.join(path_to_dataset, 'test.csv')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "data_test = pd.read_csv(path_to_chatbot)\n",
    "data_test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d7b17-ae6f-4ac9-8a9c-44d04162b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 분석 함수\n",
    "#===============================================================================================================================#\n",
    "def below_threshold_len(max_len, data, features):\n",
    "    max_len_list = []\n",
    "    # 두 입력의 길이가 맞지 않다면 맞춰주기\n",
    "    if len(max_len) < len(features):\n",
    "        max_len_list = [max_len for _ in range(len(features) - len(max_len))]\n",
    "    elif len(max_len) > len(features):\n",
    "        max_len_list = max_len[:len(features)]\n",
    "    else:\n",
    "        max_len_list = max_len\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        cnt=0\n",
    "        for s in data[feature]:\n",
    "            if(len(s.split())<=max_len_list[idx]):\n",
    "                cnt = cnt+1\n",
    "        print('전체 %s 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(feature ,max_len_list[idx], (cnt / len(data[feature]))))\n",
    "\n",
    "#데이터 길이 시각화 함수\n",
    "def DataLengthVisualization(data, features, bins=40):\n",
    "    \n",
    "    #가변 길이 처리하기위한 dictionary\n",
    "    container = dict()\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        container[feature] = [len(s.split()) for s in data[feature]]\n",
    "    \n",
    "        print('{}의 최소 길이 : {}'.format(feature, np.min(container[feature])))\n",
    "        print('{}의 최대 길이 : {}'.format(feature, np.max(container[feature])))\n",
    "        print('{}의 평균 길이 : {}'.format(feature, np.mean(container[feature])))\n",
    "    \n",
    "        plt.subplot(1,len(features),idx+1)\n",
    "        plt.boxplot(container[feature])\n",
    "        plt.title(feature)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "    \n",
    "        plt.title(feature)\n",
    "        plt.hist(container[feature],bins=bins)\n",
    "        plt.xlabel('length of samples')\n",
    "        plt.ylabel('number of samples')\n",
    "        plt.show()\n",
    "\n",
    "#희귀 단어 파악\n",
    "def spase_word(data, features, threshold=7):\n",
    "\n",
    "    #가변 길이 처리하기위한 dictionary\n",
    "    container = dict()\n",
    "    \n",
    "    for feature in features:\n",
    "        container[feature] = data[feature].tolist()\n",
    "        \n",
    "        # 단어 빈도수 계산\n",
    "        word_counter = Counter()\n",
    "        for text in container[feature]:\n",
    "            word_counter.update(text.split())\n",
    "        \n",
    "        total_cnt = len(word_counter)  # 전체 단어 개수\n",
    "        total_freq = sum(word_counter.values())  # 전체 단어 등장 횟수\n",
    "        rare_cnt = sum(1 for count in word_counter.values() if count < threshold)  # 희귀 단어 개수\n",
    "        rare_freq = sum(count for count in word_counter.values() if count < threshold)  # 희귀 단어 등장 횟수\n",
    "        \n",
    "        # 희귀 단어를 제외한 단어 사전 구축\n",
    "        vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # 패딩 및 미등록 단어 추가\n",
    "        word_index = {word: idx + 2 for idx, (word, count) in enumerate(word_counter.items()) if count >= threshold}\n",
    "\n",
    "        print('대상 feature : ', feature)\n",
    "        print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "        print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "        print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "        print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "        print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "        print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738472d0-4c08-4cdc-be4c-c055552f1d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "#==========================================================================================================================================================================\n",
    "def preprocess_sentence(sentence):\n",
    "    # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "    # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def preprocessing(data, features, corpus_file, max_samples = 40000, insert_aug=False, max_aug=1, aug_cycle=1):\n",
    "    data.drop_duplicates(subset=\"idx\", inplace=True)\n",
    "    if data.isnull().sum().sum() > 0:\n",
    "        data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    if insert_aug:\n",
    "        for j in range(aug_cycle):\n",
    "            temp_data = data\n",
    "            indices = np.arange(temp_data.shape[0]-1)\n",
    "            np.random.shuffle(indices)\n",
    "            cls_list=[]\n",
    "            text_list=[]\n",
    "            idx_list=[]\n",
    "            temp_data = pd.DataFrame(temp_data.to_numpy()[1:][indices], columns = data.columns)\n",
    "            for i, (cls, text) in enumerate(zip(tqdm(temp_data[features[0]]), temp_data[features[1]])):\n",
    "                # klue/bert-base, beomi/kcbert-base\n",
    "                if max_aug is not None and max_aug <= 0:\n",
    "                    break\n",
    "                aug = naw.ContextualWordEmbsAug(model_path='beomi/kcbert-large', action='insert', aug_p = min(0.4, 0.3), aug_min=1, device=device)\n",
    "                aug_text = str(aug.augment(text)[0])\n",
    "                cls_list.append(cls)\n",
    "                text_list.append(aug_text+ \"\\n\")\n",
    "                idx_list.append(i+len(data))\n",
    "                if max_aug is not None and (i+1) >= max_aug:\n",
    "                    break\n",
    "    \n",
    "            aug_data = pd.DataFrame({'idx': idx_list,'class': cls_list, 'conversation': text_list})\n",
    "            temp_data = aug_data.copy()\n",
    "            data = pd.concat([data, aug_data], ignore_index=True)\n",
    "            print(f\"Augmentation cycle : {j+1}\")\n",
    "\n",
    "    for feature in features:\n",
    "        data[feature] = data[feature].apply(preprocess_sentence)\n",
    "\n",
    "    data.drop_duplicates(subset=\"conversation\", inplace=True)\n",
    "    \n",
    "    with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "        for conv in data[features[1]]:\n",
    "            f.write(conv + \"\\n\")\n",
    "    if data.shape[0] < max_samples:\n",
    "        return data[features]\n",
    "    else:\n",
    "        return data[features].iloc[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc57fc-b58f-40b4-a772-baf86c87c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 정의\n",
    "#========================================================================================================================================================================\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, sp, num_classes=4, max_length=300, features=None, random_mask_ratio=0.0, isDecoder=False):\n",
    "        super().__init__()\n",
    "        self.sp = sp\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        self.random_mask_ratio = random.random() * random_mask_ratio\n",
    "        self.mask_id = sp.PieceToId('<mask>') if sp.PieceToId('<mask>') >= 0 else 4\n",
    "        self.isDecoder = isDecoder\n",
    "\n",
    "        if features is None:\n",
    "            features = data.columns[:-2]\n",
    "\n",
    "        for target, text in zip(data[features[0]], data[features[1]]):\n",
    "            # 1) tokenize\n",
    "            text_ids = sp.EncodeAsIds(text)\n",
    "\n",
    "            # 2) [CLS]/[SEP] 같은 별도 스페셜 토큰을 쓸 수도 있으나,\n",
    "            #    여기서는 SentencePiece 기본 <s>, </s> 등 혹은 사용자 정의 토큰 활용 가능\n",
    "            #    간단히 <s>=sp.bos_id(), </s>=sp.eos_id()로 가정해본다면:\n",
    "            #    sp.SetEncodeExtraOptions(\"bos:eos\") 등으로 설정하는 방법도 있음.\n",
    "            # 여기서는 수동으로 bos/eos id를 붙인다고 가정\n",
    "            bos_id = sp.bos_id() if sp.bos_id() >= 0 else 1\n",
    "            eos_id = sp.eos_id() if sp.eos_id() >= 0 else 2\n",
    "\n",
    "            text_tokens = [bos_id] + text_ids + [eos_id]\n",
    "            original_len = len(text_tokens)\n",
    "\n",
    "            # 3) 길이 제한\n",
    "            if len(text_tokens) > max_length:\n",
    "                continue\n",
    "\n",
    "            # 4) 고정 길이 패딩\n",
    "            text_tokens += [0]*(max_length - len(text_tokens))  # 0 -> <pad> 가정\n",
    "\n",
    "            self.data.append({\n",
    "                \"text_input\":text_tokens,\n",
    "                \"target\":target,\n",
    "                \"original_len\": original_len\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.isDecoder:\n",
    "            radom_count = 0\n",
    "        else:\n",
    "            radom_count = int(sample[\"original_len\"]*self.random_mask_ratio)\n",
    "        if radom_count > 0:\n",
    "            ramdom_index = list(np.random.choice(range(1, sample[\"original_len\"]-1), radom_count, replace=False))\n",
    "            masked_input = sample[\"text_input\"].copy()\n",
    "            for i in range(radom_count):\n",
    "                masked_input[ramdom_index[i]] = self.mask_id  \n",
    "            text_input = torch.tensor(masked_input, dtype=torch.long)\n",
    "        else:\n",
    "            text_input = torch.tensor(sample[\"text_input\"], dtype=torch.long)\n",
    "        target = torch.tensor(sample[\"target\"], dtype=torch.long)\n",
    "        if self.isDecoder:\n",
    "            text = torch.tensor(sample[\"text_input\"][:-1], dtype=torch.long)\n",
    "        else:\n",
    "            text = torch.tensor(sample[\"text_input\"], dtype=torch.long)\n",
    "        return text_input, target, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad154f-4663-4980-9073-936776288a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_longrope_parameters(\n",
    "    head_dim,\n",
    "    max_position_embeddings,\n",
    "    device: \"torch.device\",\n",
    "    seq_len: Optional[int] = None,\n",
    "    layer_type: Optional[str] = None,\n",
    "    rope_theta: int = 10000,\n",
    "    long_factor: float = 4.0,\n",
    "    short_factor: float = 1.0,\n",
    "    partial_rotary_factor:float = 1.0,\n",
    "    original_max_position_embeddings= None,\n",
    ") -> tuple[\"torch.Tensor\", float]:\n",
    "    \"\"\"\n",
    "    Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n",
    "    [original implementation](https://github.com/microsoft/LongRoPE)\n",
    "\n",
    "    Args:\n",
    "            The model configuration. This function assumes that the config will provide at least the following\n",
    "            properties:\n",
    "\n",
    "            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n",
    "            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n",
    "            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n",
    "            *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n",
    "            *   original_max_position_embeddings (`int`, *optional*): The original max position embeddings used during\n",
    "                pretraining. If not provided, defaults to `max_position_embeddings`.\n",
    "            *   rope_parameters (`dict[str, float]`): The standard RoPE scaling parameters, from which the following keys\n",
    "                will be accessed:\n",
    "                *   `attention_factor` (`float`, *optional*): The scaling factor to be applied on the attention\n",
    "                    computation. If unspecified, it defaults to value recommended by the implementation, inferred from\n",
    "                    the value of `factor`.\n",
    "                *   `factor` (`float`, *optional*): The scaling factor to apply to the RoPE embeddings. If both\n",
    "                    `max_position_embeddings` and `original_max_position_embeddings` are provided, this value will be\n",
    "                    overridden s the ratio between those values.\n",
    "                *   `long_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n",
    "                    frequencies if `seq_len` is provided and greater than `original_max_position_embeddings`.\n",
    "                *   `short_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n",
    "                    frequencies if `seq_len` is None or less-than-or-equal-to `original_max_position_embeddings`.\n",
    "\n",
    "            Additionally, this function will make use of the following properties if they are found in the config:\n",
    "\n",
    "            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n",
    "                derived as hidden_size // num_attention_heads.\n",
    "            *   partial_rotary_factor (`float`, *optional*, defaults to 1.0): If less than 1.0, inverse frequencies\n",
    "                will be returned for the first fraction of the head_dim.\n",
    "        device (`torch.device`):\n",
    "            The device to use for initialization of the inverse frequencies.\n",
    "        seq_len (`int`, *optional*):\n",
    "            The current sequence length.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n",
    "        post-processing scaling factor applied to the computed cos/sin.\n",
    "    \"\"\"\n",
    "    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n",
    "    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n",
    "    \n",
    "    base = rope_theta\n",
    "    dim = int(head_dim * partial_rotary_factor)\n",
    "    long_factor = long_factor\n",
    "    short_factor = short_factor\n",
    "    attention_factor = 1.0\n",
    "\n",
    "    # NOTE: Phi3 (and potentially other models) modify `max_position_embeddings` and have a\n",
    "    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n",
    "    # values to compute the default attention scaling factor, instead of using `factor`.\n",
    "    if original_max_position_embeddings is not None:\n",
    "        factor = max_position_embeddings / original_max_position_embeddings\n",
    "    else:\n",
    "        original_max_position_embeddings = max_position_embeddings\n",
    "        factor = 1.0\n",
    "\n",
    "    # Sets the attention factor as suggested in the paper\n",
    "    if attention_factor is None:\n",
    "        if factor <= 1.0:\n",
    "            attention_factor = 1.0\n",
    "        else:\n",
    "            attention_factor = math.sqrt(1 + math.log(factor) / math.log(original_max_position_embeddings))\n",
    "\n",
    "    # Compute the inverse frequencies -- scaled based on the target sequence length\n",
    "    if seq_len and seq_len > original_max_position_embeddings:\n",
    "        ext_factors = torch.tensor(long_factor, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        ext_factors = torch.tensor(short_factor, dtype=torch.float32, device=device)\n",
    "        \n",
    "    inv_freq_shape = torch.arange(0, dim, 2, dtype=torch.int64, device=device).float() / dim\n",
    "    inv_freq = 1.0 / (ext_factors * base**inv_freq_shape)\n",
    "\n",
    "    return inv_freq, attention_factor\n",
    "\n",
    "def _compute_linear_scaling_rope_parameters(\n",
    "    head_dim,\n",
    "    rope_theta: int = 10000,\n",
    "    device: Optional[\"torch.device\"] = None,\n",
    "    seq_len: Optional[int] = None,\n",
    "    layer_type: Optional[str] = None,\n",
    ") -> tuple[\"torch.Tensor\", float]:\n",
    "    \"\"\"\n",
    "    Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n",
    "    Args:\n",
    "        config ([`~transformers.PreTrainedConfig`]):\n",
    "            The model configuration. This function assumes that the config will provide at least the following\n",
    "            properties:\n",
    "\n",
    "            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n",
    "            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n",
    "            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n",
    "\n",
    "            Additionally, this function will make use of the following properties if they are found in the config:\n",
    "\n",
    "            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n",
    "                derived as hidden_size // num_attention_heads.\n",
    "            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n",
    "                the first fraction of the head_dim. Defaults to 1.0.\n",
    "        device (`torch.device`):\n",
    "            The device to use for initialization of the inverse frequencies.\n",
    "        seq_len (`int`, *optional*):\n",
    "            The current sequence length. Unused for this type of RoPE.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n",
    "        post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n",
    "    \"\"\"\n",
    "    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n",
    "    factor = 1.0\n",
    "\n",
    "    # Gets the default RoPE parameters\n",
    "    base = rope_theta\n",
    "    partial_rotary_factor = 1.0\n",
    "    dim = int(head_dim * partial_rotary_factor)\n",
    "    attention_factor = 1.0  # Unused in this type of RoPE\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n",
    "\n",
    "    # Then applies linear scaling to the frequencies.\n",
    "    # NOTE: originally, scaling was applied to the position_ids. However, we get `embs = inv_freq @ position_ids`, so\n",
    "    # applying scaling to the inverse frequencies is equivalent.\n",
    "    inv_freq /= factor\n",
    "    return inv_freq, attention_factor\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, max_position_embeddings, head_dim, position, rope_type=\"default\", device=None):\n",
    "        super().__init__()\n",
    "        self.rope_type = rope_type\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "        self.original_max_seq_len = max_position_embeddings\n",
    "        self.position = position\n",
    "\n",
    "        self.rope_init_fn = _compute_linear_scaling_rope_parameters\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(head_dim = head_dim, seq_len = position, device = device)\n",
    "        \n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    def _dynamic_frequency_update(self, position, device):\n",
    "        \"\"\"\n",
    "        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n",
    "        1 - growing beyond the cached sequence length (allow scaling)\n",
    "        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n",
    "        \"\"\"\n",
    "        if position > self.max_seq_len_cached:  # growth\n",
    "            inv_freq, self.attention_scaling = self.rope_init_fn(device, head_dim = head_dim, seq_len=position)\n",
    "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n",
    "            self.max_seq_len_cached = position\n",
    "\n",
    "        if position < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n",
    "            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n",
    "            self.max_seq_len_cached = self.original_max_seq_len\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position, batch_size, device: Optional[\"torch.device\"] = None,):\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        \n",
    "        past = 0  # KV 캐시 길이 등 상황에 따라 다름\n",
    "        position_ids = torch.arange(past, past + position, device=device_type)[None, :].expand(batch_size, -1)\n",
    "        \n",
    "        if \"dynamic\" in self.rope_type:\n",
    "            self._dynamic_frequency_update(position_ids, device=x.device)\n",
    "\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        \n",
    "        # FP16 FP32 사용\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling\n",
    "        sin = sin * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54bcd3-c4d2-429c-995f-d0b94d812663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer model 설계\n",
    "#============================================================================================================================================================================\n",
    "\n",
    "def create_padding_mask(x):\n",
    "    # x == 0 위치를 찾아 float형 1로 변환\n",
    "    mask = (x==0).float()\n",
    "    # (batch_size, seq_len) -> (batch_size, 1, 1, seq_len)\n",
    "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "    return mask\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = x.size(1)\n",
    "\n",
    "    # (seq_len, seq_len) 크기의 하삼각 행렬(tril) 생성 후 1에서 빼서\n",
    "    # 상삼각이 1, 하삼각(자기 자신 포함)이 0이 되도록 설정\n",
    "    # => 미래 토큰(자신 인덱스보다 큰 위치) 마스킹\n",
    "    look_ahead_mask = 1 - torch.tril(torch.ones((seq_len, seq_len)))\n",
    "\n",
    "    # 패딩 마스크 생성 (shape: (batch_size, 1, 1, seq_len))\n",
    "    padding_mask = create_padding_mask(x)\n",
    "\n",
    "    # look_ahead_mask: (seq_len, seq_len) -> (1, seq_len, seq_len)  -> (1, 1, seq_len, seq_len)\n",
    "    look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(1)\n",
    "    look_ahead_mask = look_ahead_mask.to(x.device)\n",
    "\n",
    "    # look-ahead 마스크와 패딩 마스크를 합성 (둘 중 하나라도 1이면 마스킹)\n",
    "    # 최종 shape은 브로드캐스팅으로 (batch_size, 1, seq_len, seq_len)\n",
    "    combined_mask = torch.max(look_ahead_mask, padding_mask)\n",
    "    return combined_mask\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "\n",
    "    # 1) Q와 K의 내적을 통해 score(유사도) 계산\n",
    "    # key.transpose(-1, -2): (batch_size, heads, depth, seq_len)\n",
    "    # matmul 결과 shape: (batch_size, heads, seq_len, seq_len)\n",
    "    matmul_qk=torch.matmul(query, key.transpose(-1, -2))\n",
    "    \n",
    "    # 2) depth에 따라 정규화\n",
    "    depth = key.size(-1) # depth = d_model / heads\n",
    "    logits = matmul_qk / math.sqrt(depth)\n",
    "\n",
    "    # 3) 마스크가 주어졌다면 -1e9(아주 작은 값)를 더해 소프트맥스에서 제외시키도록 함\n",
    "    # 아주 작은 값이 더해지면 e^(logit+epsilon) 가 되기 때문에 e^logit * e^epsilon 이 되어 무시할 수 있는 값이 됨\n",
    "    if mask is not None:\n",
    "        logits = logits + (mask * -1e9)\n",
    "\n",
    "    # 4) 소프트맥스 계산해 attention weights 생성\n",
    "    attention_weights = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # 5) attention weights와 value의 내적\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # d_model은 num_heads로 나누어떨어져야 함\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # 파이토치에서 Dense는 nn.Linear로 대응\n",
    "        self.query_dense = nn.Linear(d_model, d_model)\n",
    "        self.key_dense = nn.Linear(d_model, d_model)\n",
    "        self.value_dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        => (batch_size, num_heads, seq_len, depth) 형태로 변환\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        x = x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, depth)\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key, value, position_embeddings=None, mask=None):\n",
    "        \"\"\"\n",
    "        query, key, value: (batch_size, seq_len, d_model)\n",
    "        mask: (batch_size, 1, seq_len, seq_len) 등으로 broadcast 가능하도록 구성\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Q, K, V에 각각 Linear 적용\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # Head 분할\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        if position_embeddings is not None:\n",
    "            cos, sin = position_embeddings\n",
    "            query_RoPE, key_RoPE = apply_rotary_pos_emb(query, key, cos, sin)\n",
    "            # 스케일드 닷 프로덕트 어텐션\n",
    "            scaled_attention, _ = scaled_dot_product_attention(query_RoPE, key_RoPE, value, mask)\n",
    "        else:\n",
    "            # 스케일드 닷 프로덕트 어텐션\n",
    "            scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, depth) -> (batch_size, seq_len, num_heads, depth)\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # 다시 (batch_size, seq_len, d_model)로 합치기\n",
    "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = self.out_dense(concat_attention)\n",
    "        return output\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, vocab_size, max_len, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  # 이전에 구현한 MHA\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(max_position_embeddings=max_len, head_dim=d_model // num_heads, position=vocab_size, device=device)\n",
    "\n",
    "        # 피드포워드 부분 (Dense -> SiLU -> Dense)\n",
    "        # value branch\n",
    "        self.w_v = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        # gate branch\n",
    "        self.w_g = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "              \n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.w_out = nn.Linear(ff_dim, d_model)\n",
    "        nn.init.zeros_(self.w_out.weight)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        position_embeddings = self.rotary_emb(x=x, position= x.shape[1], batch_size=x.shape[0])\n",
    "        \n",
    "        # (1) 멀티 헤드 어텐션 (셀프 어텐션)\n",
    "        x = self.norm1(x)\n",
    "        attn_output = self.mha(x, x, x, position_embeddings, mask)  # (batch_size, seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = x + attn_output     # 잔차 연결 + LayerNorm\n",
    "\n",
    "        # (2) 피드포워드 신경망\n",
    "        # SwiGLU\n",
    "        out1 = self.norm2(out1)\n",
    "        v = self.w_v(out1)           # (batch_size, seq_len, d_model)\n",
    "        g = self.w_g(out1)\n",
    "        h = self.dropout3(v * g)\n",
    "        ffn_output = self.w_out(h)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = out1 + ffn_output   # 잔차 연결 + LayerNorm\n",
    "\n",
    "        return out2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 ff_dim,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 max_len,\n",
    "                 dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # (1) 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        # (3) EncoderLayer 쌓기\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, ff_dim, vocab_size, max_len, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # (1) 임베딩 & sqrt(d_model)로 스케일링\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # (3) num_layers만큼 쌓아올린 EncoderLayer 통과\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 units,\n",
    "                 d_model,\n",
    "                 target_size,\n",
    "                 num_heads,\n",
    "                 max_len,\n",
    "                 dropout=0.1, \n",
    "                 aux_available=False):\n",
    "\n",
    "        super(EncoderBase, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            vocab_size = vocab_size,\n",
    "            num_layers = num_layers,\n",
    "            ff_dim = units,\n",
    "            d_model = d_model,\n",
    "            num_heads = num_heads,\n",
    "            max_len = max_len,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        self.aux_available = aux_available\n",
    "        \n",
    "        # 최종 출력층: (d_model) -> (target_size)\n",
    "        self.final_linear = nn.Linear(d_model, target_size, bias=False)\n",
    "        if aux_available:\n",
    "            self.auxilary_linear = nn.Linear(d_model, vocab_size, bias=False) # vocab_size\n",
    "            self.auxilary_linear.weight = self.encoder.embedding.weight\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # 1) 인코더 패딩 마스크 생성\n",
    "        enc_padding_mask = create_padding_mask(inputs) # shape (batch_size, 1, 1, src_seq_len)\n",
    "\n",
    "        # 4) 인코더 수행\n",
    "        enc_outputs = self.encoder(\n",
    "            x = inputs,\n",
    "            mask = enc_padding_mask\n",
    "        )  # shape: (batch_size, d_model)\n",
    "\n",
    "        if self.aux_available:\n",
    "            self.auxilary_linear.weight = self.encoder.embedding.weight\n",
    "    \n",
    "            logits_aux = self.auxilary_linear(enc_outputs)\n",
    "        \n",
    "        logits = self.final_linear(enc_outputs.mean(dim=1)) # shape: (batch_size, target_size)\n",
    "        if self.aux_available:\n",
    "            return logits, logits_aux\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ae56a-e6ca-4dc5-9012-a60349547f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder base는 pretraining 고려\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, vocab_size, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  # 이전에 구현한 MHA\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(max_position_embeddings=vocab_size, head_dim=d_model // num_heads, position=vocab_size, device=device)\n",
    "\n",
    "        # 피드포워드 부분 (Dense -> SiLU -> Dense)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x, look_ahead_mask=None, padding_mask=None):\n",
    "        position_embeddings = self.rotary_emb(x=x, position= x.shape[1], batch_size=x.shape[0])\n",
    "        \n",
    "        # (1) 멀티 헤드 어텐션 (셀프 어텐션)\n",
    "        attn_output = self.mha(x, x, x, position_embeddings, mask=look_ahead_mask)  # (batch_size, seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.norm1(x + attn_output)     # 잔차 연결 + LayerNorm\n",
    "\n",
    "        # (2) 피드포워드 신경망\n",
    "        ffn_output = self.ffn(out1)            # (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.norm2(out1 + ffn_output)   # 잔차 연결 + LayerNorm\n",
    "\n",
    "        return out2\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 ff_dim,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # (1) 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # (3) DecoderLayer 쌓기\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, ff_dim, vocab_size, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # (1) 임베딩 & sqrt(d_model)로 스케일링\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # (3) num_layers만큼 쌓아올린 DecoderLayer 통과\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, look_ahead_mask, padding_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 units,\n",
    "                 d_model,\n",
    "                 target_size,\n",
    "                 num_heads,\n",
    "                 dropout=0.1):\n",
    "\n",
    "        super(EncoderBase, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size = vocab_size,\n",
    "            num_layers = num_layers,\n",
    "            ff_dim = units,\n",
    "            d_model = d_model,\n",
    "            num_heads = num_heads,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "\n",
    "        # 최종 출력층: (d_model) -> (target_size)\n",
    "        self.final_linear = nn.Linear(d_model, target_size)\n",
    "        self.auxilary_linear = nn.Linear(d_model, vocab_size)\n",
    "        self.auxilary_linear.weight = self.encoder.embedding.weight\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # 2) 디코더 look-ahead + 패딩 마스크\n",
    "        look_ahead_mask = create_look_ahead_mask(dec_inputs) # shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        # 4) 인코더 수행\n",
    "        dec_outputs = self.decoder(\n",
    "            x = inputs,\n",
    "            look_ahead_mask = look_ahead_mask,\n",
    "        )  # shape: (batch_size, tgt_seq_len, d_model)\n",
    "        \n",
    "        self.auxilary_linear.weight = self.encoder.embedding.weight\n",
    "\n",
    "        logits_aux = self.auxilary_linear(dec_outputs)\n",
    "        \n",
    "        logits = self.final_linear(dec_outputs.mean(dim=1)) # shape: (batch_size, tgt_seq_len, target_size)\n",
    "        return logits, logits_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b8ac4-1993-458a-a055-0a68e9eef62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 및 보조 함수/ 기타\n",
    "#=================================================================================================================================================================================\n",
    "\n",
    "def Loss_Visualization(train_losses, val_losses):\n",
    "    plt.plot(range(len(train_losses)), train_losses, 'b-',label='Train Loss')\n",
    "    plt.plot(range(len(val_losses)), val_losses,'r--', label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.show()\n",
    "\n",
    "def Acc_Visualization(train_acc, val_acc):\n",
    "    plt.plot(range(len(train_acc)), train_acc, 'b-',label='Train Acc')\n",
    "    plt.plot(range(len(val_acc)), val_acc,'r--', label='Validation Acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Acc\")\n",
    "    plt.title(\"Training and Validation Acc\")\n",
    "    plt.show()\n",
    "\n",
    "def F1_Visualization(train_f1, val_f1):\n",
    "    plt.plot(range(len(train_f1)), train_f1, 'b-',label='Train F1')\n",
    "    plt.plot(range(len(val_f1)), val_f1,'r--', label='Validation F1')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.title(\"Training and Validation F1\")\n",
    "    plt.show()\n",
    "\n",
    "def accuracy_function(y_pred, y_true, isMultiLabel=False, pad_id=0):\n",
    "    \"\"\"\n",
    "    y_pred: (batch_size, target_size)\n",
    "    y_true: (batch_size,)\n",
    "    \"\"\"\n",
    "    preds = y_pred.argmax(dim=-1)\n",
    "    if isMultiLabel:\n",
    "        mask = (y_true != pad_id)\n",
    "        correct = (preds == y_true) & mask\n",
    "        acc = correct.float().sum() / mask.float().sum()\n",
    "    else:\n",
    "        acc = (preds == y_true).float().mean()\n",
    "    return acc\n",
    "\n",
    "def f1_function(y_pred, y_true, num_classes):\n",
    "    \"\"\"\n",
    "    y_pred: (batch_size, target_size)\n",
    "    y_true: (batch_size, )\n",
    "    \"\"\"\n",
    "    preds = y_pred.argmax(dim=-1)\n",
    "\n",
    "    f1_metric = MulticlassF1Score(num_classes=num_classes, average='none').to(device)\n",
    "\n",
    "    f1_scores = f1_metric(preds, y_true).mean()\n",
    "    \n",
    "    return f1_scores\n",
    "\n",
    "def get_lr_lambda(d_model, warmup_steps = 4000):\n",
    "    d_model = float(d_model)\n",
    "    def lr_lambda(step):\n",
    "        step += 1\n",
    "        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
    "\n",
    "    return lr_lambda\n",
    "\n",
    "def Label_order(data, disordered, ordered, target_column):\n",
    "    disordered = le.classes_\n",
    "    ordered = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일상대화']\n",
    "    mapping = {idx: ordered.index(label) for idx, label in enumerate(disordered)}\n",
    "\n",
    "    data.replace({target_column: mapping}, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02826fbb-83ba-4ae8-a7ba-04fbe860a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "#=======================================================================================================================================================================\n",
    "def train_model(model, train_dataset,vocab_size=8000,\n",
    "                num_layers=2, units=512, d_model=256,\n",
    "                num_heads=8, dropout=0.1, train_ratio=0.8,\n",
    "                warmup_steps=4000, criterion = 'CE', optimize='Adam',\n",
    "                batch_size=256, epochs=50,lr = 0.001,  scheduler='Warmup_cosine',\n",
    "                verbose = 1, patience=4, max_len = 40, isTrained = False, _lambda=0.5, random_state=None,\n",
    "                alpha=1.0, save_path = None, model_name=None, view_accurate_value = False,\n",
    "                view_aux=False, aux_available=False):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if isTrained:\n",
    "        model.to(device)\n",
    "    else:\n",
    "        model = model(\n",
    "            vocab_size=vocab_size,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            units=UNITS,\n",
    "            d_model=D_MODEL,\n",
    "            target_size = NUM_CLASSES,\n",
    "            num_heads=NUM_HEADS,\n",
    "            max_len = max_len,\n",
    "            dropout=DROPOUT,\n",
    "            aux_available = aux_available\n",
    "        )\n",
    "    \n",
    "        model.to(device)\n",
    "\n",
    "    # 손실 함수\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_aux = nn.CrossEntropyLoss(ignore_index=0) # pad 무시 (0 가정)\n",
    "\n",
    "    # 옵티마이저\n",
    "    if optimize=='AdamW':\n",
    "        optimizer=optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif optimize=='Adam':\n",
    "        optimizer=optim.Adam(model.parameters(), betas = (0.9, 0.98), eps=1e-9, lr=lr)\n",
    "    elif optimize==\"SGD\":\n",
    "        optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer=optim.Adam(model.parameters(), betas = (0.9, 0.98), eps=1e-9, lr=lr)\n",
    "\n",
    "    train_size = int(len(train_dataset) * train_ratio)\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    if random_state is None:\n",
    "        # 랜덤하게 dataset나누기\n",
    "        tr_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    else:\n",
    "        manual_seed = random_state\n",
    "        generator = torch.Generator().manual_seed(manual_seed)\n",
    "    \n",
    "        # 랜덤하게 dataset나누기\n",
    "        tr_dataset, val_dataset = random_split(train_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "    if scheduler==\"Warmup_cosine\":\n",
    "        sched_warmup = LinearLR(optimizer, start_factor=1e-3, end_factor=1.0, total_iters=max(1, warmup_steps))\n",
    "        sched_cosine = CosineAnnealingLR(optimizer, T_max=max(1, train_size - warmup_steps), eta_min=1e-6)\n",
    "        scheduler_ = SequentialLR(optimizer, schedulers=[sched_warmup, sched_cosine], milestones=[warmup_steps])\n",
    "    elif scheduler==\"LambdaLR\":\n",
    "        scheduler_ = lr_scheduler.LambdaLR(optimizer, lr_lambda = get_lr_lambda(d_model, warmup_steps = 4000))\n",
    "    elif scheduler==\"OneCycleLR\":\n",
    "        scheduler_ = OneCycleLR(optimizer, max_lr=3e-4, steps_per_epoch=train_size, epochs=epochs, pct_start=0.1, anneal_strategy='cos', div_factor=25.0, final_div_factor=1e3)\n",
    "    else:\n",
    "        scheduler_ = lr_scheduler.LambdaLR(optimizer, lr_lambda = get_lr_lambda(d_model, warmup_steps = 4000))\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    train_f1_list = []\n",
    "    val_f1_list = []\n",
    "    train_aux_acc_list = []\n",
    "    val_aux_acc_list = []\n",
    "\n",
    "    model.train()\n",
    "    best_param = dict()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, total_f1, total_acc, total_aux_acc = 0, 0, 0, 0\n",
    "\n",
    "        # PyTorch DataLoader 설정. epoch 마다 train과 test 에서\n",
    "        train_loader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss, f1, acc, acc_aux = train_step(model, batch, optimizer, criterion, criterion_aux, device, aux_target_label=4, _lambda=_lambda, alpha=alpha, num_classes=NUM_CLASSES, aux_available=aux_available)\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            total_f1 += f1\n",
    "            total_aux_acc += acc_aux\n",
    "\n",
    "            scheduler_.step()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_acc = total_acc / len(train_loader)\n",
    "        avg_f1 = total_f1 / len(train_loader)\n",
    "        avg_aux_acc = total_aux_acc / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        train_acc_list.append(avg_acc.to('cpu'))\n",
    "        train_f1_list.append(avg_f1.to('cpu'))\n",
    "        train_aux_acc_list.append(avg_aux_acc.to('cpu'))\n",
    "\n",
    "        # Validation loss 계산\n",
    "        model.eval()\n",
    "        val_loss, val_f1, val_acc, val_aux_acc = 0, 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                loss, f1, acc, acc_aux = eval_step(model, batch, criterion, criterion_aux, device, aux_target_label=4,_lambda= _lambda, alpha=alpha, num_classes=NUM_CLASSES, aux_available=aux_available)\n",
    "\n",
    "                val_loss += loss\n",
    "                val_acc += acc\n",
    "                val_f1 += f1\n",
    "                val_aux_acc += acc_aux\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc /= len(val_loader)\n",
    "        val_acc_list.append(val_acc.to('cpu'))\n",
    "        val_f1 /= len(val_loader)\n",
    "        val_f1_list.append(val_f1.to('cpu'))\n",
    "        val_aux_acc /= len(val_loader)\n",
    "        val_aux_acc_list.append(val_aux_acc.to('cpu'))\n",
    "\n",
    "        if (epoch+1)%verbose==0:\n",
    "            if view_accurate_value:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss}, Train F1: {avg_f1}, Train Acc: {avg_acc} | Val Loss: {val_loss}, Val F1: {val_f1}, Val Acc: {val_acc}\")\n",
    "                if view_aux:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} |Train Aux Acc: {avg_aux_acc} |Val Aux Acc: {val_aux_acc}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f}, Train F1: {avg_f1:.4f}, Train Acc: {avg_acc:.4f} | Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "                if view_aux:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} |Train Aux Acc: {avg_aux_acc:.4f} |Val Aux Acc: {val_aux_acc:.4f}\")\n",
    "                    \n",
    "        # Early Stopping 조건\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_param = copy.deepcopy(model.state_dict())\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f}, Train F1: {avg_f1:.4f}, Train Acc: {avg_acc:.4f} | Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            if save_path is not None:\n",
    "                if model_name is None:\n",
    "                    model.load_state_dict(best_param)\n",
    "                    torch.save(model.state_dict(), os.path.join(save_path, \"{}_Encode_Base_RoPE_Linear_{}_Early_Stopped_{}.pt\".format(datetime.now().strftime(\"%Y_%m_%d_%H_%M\"), epochs, epoch+1)))\n",
    "                else:\n",
    "                    model.load_state_dict(best_param)\n",
    "                    torch.save(model.state_dict(), os.path.join(save_path, \"{}_{}_{}_Early_Stopped_{}\".format(datetime.now().strftime(\"%Y_%m_%d_%H_%M\"), model_name, epochs, epoch+1)))\n",
    "            \n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    model.load_state_dict(best_param)\n",
    "\n",
    "    Loss_Visualization(train_losses, val_losses)\n",
    "    Acc_Visualization(train_acc_list, val_acc_list)\n",
    "    F1_Visualization(train_f1_list, val_f1_list)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        if model_name is None:\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, \"{}_Encode_Base_RoPE_Linear_{}.pt\".format(datetime.now().strftime(\"%Y_%m_%d_%H_%M\"), epochs)))\n",
    "        else:\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, \"{}_{}_{}\".format(datetime.now().strftime(\"%Y_%m_%d_%H_%M\"), model_name, epochs)))\n",
    "    if not isTrained:\n",
    "        return model\n",
    "\n",
    "def train_step(model, batch, optimizer, loss_function, loss_function_aux, device, aux_target_label, _lambda=0.5, alpha=1.0,num_classes=5, aux_available=False):\n",
    "    model.train()\n",
    "    test_input, target, text = [x.to(device) for x in batch]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if aux_available:\n",
    "        # 모델 포워드 패스\n",
    "        logits, logits_aux  = model(test_input) # (batch_size, target_size) or (batch_size, seq_len, vocab_size)\n",
    "    \n",
    "        # Loss 계산\n",
    "        loss = alpha * loss_function(logits, target) + _lambda * loss_function(logits_aux.permute(0, 2, 1), text) # (batch_size, target_size)\n",
    "    \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        return loss.item(), f1_function(logits, target, num_classes = num_classes), accuracy_function(logits, target), accuracy_function(logits_aux, text, isMultiLabel=True)\n",
    "    else:\n",
    "         # 모델 포워드 패스\n",
    "        logits = model(test_input) # (batch_size, target_size) or (batch_size,seq_len, vocab_size)\n",
    "    \n",
    "        # Loss 계산\n",
    "        loss = alpha * loss_function(logits, target)\n",
    "    \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        return loss.item(), f1_function(logits, target, num_classes = num_classes), accuracy_function(logits, target), 0\n",
    "    \n",
    "def eval_step(model, batch, loss_function, loss_function_aux, device, aux_target_label, _lambda=0.5, alpha=1.0,num_classes=5, aux_available=False):\n",
    "    model.eval()\n",
    "    test_input, target, text = [x.to(device) for x in batch]\n",
    "\n",
    "    if aux_available:\n",
    "       # 모델 포워드 패스\n",
    "        logits, logits_aux  = model(test_input) # (batch_size, target_size) or (batch_size, seq_len, vocab_size)\n",
    "    \n",
    "        # Loss 계산\n",
    "        loss = alpha * loss_function(logits, target) + _lambda * loss_function(logits_aux.permute(0, 2, 1), text) # (batch_size, target_size)\n",
    "    \n",
    "        return loss.item(), f1_function(logits, target, num_classes = num_classes), accuracy_function(logits, target), accuracy_function(logits_aux, text, isMultiLabel=True)\n",
    "    else:\n",
    "         # 모델 포워드 패스\n",
    "        logits = model(test_input) # (batch_size, target_size) or (batch_size,seq_len, vocab_size)\n",
    "    \n",
    "        # Loss 계산\n",
    "        loss = alpha * loss_function(logits, target)\n",
    "    \n",
    "        return loss.item(), f1_function(logits, target, num_classes = num_classes), accuracy_function(logits, target), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2be8c-dc9c-4740-9869-e54f82fac5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 함수\n",
    "# ==============================================================================================================================\n",
    "\n",
    "def inference(model, sentence, tokenizer, encoder, max_len, device=\"cpu\", isClassNo=False, verbose=0):\n",
    "    START_TOKEN = tokenizer.bos_id()\n",
    "    END_TOKEN = tokenizer.eos_id()\n",
    "    MAX_LENGTH = 40\n",
    "    \n",
    "    # 전처리\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 인코더 입력: [START] + 인코딩 + [END]\n",
    "    enc_input_ids = [START_TOKEN] + tokenizer.encode(sentence) + [END_TOKEN]\n",
    "\n",
    "    if len(enc_input_ids) < max_len:\n",
    "        enc_input_ids += [0]*(max_len - len(enc_input_ids))\n",
    "    elif len(enc_input_ids) > max_len:\n",
    "        enc_input_ids = enc_input_ids[:max_len-1] + [END_TOKEN]\n",
    "        \n",
    "    # 차원 확장: (batch_size=1, seq_len)\n",
    "    enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():       \n",
    "        # 모델 forward: (enc_input) -> (batch_size=1, target_size)\n",
    "        logits, logits_aux = model(enc_input)\n",
    "        if verbose > 0:\n",
    "            print(logits_aux)\n",
    "        # argmax로 가장 높은 확률의 토큰 선택\n",
    "        predicted_id = torch.argmax(logits, dim=-1).to('cpu')\n",
    "        if isClassNo:\n",
    "            predicted_class = predicted_id\n",
    "        else:\n",
    "            predicted_class = encoder.inverse_transform(predicted_id)\n",
    "            \n",
    "    return predicted_class\n",
    "\n",
    "def data_inference(model, data, tokenizer, encoder, max_len, device=\"cpu\", isClassNo=False):\n",
    "    data_submission = data.copy()\n",
    "    label = []\n",
    "    for text in data[\"conversation\"]:\n",
    "        label.append(int(inference(model, text, tokenizer, encoder, max_len, device=device, isClassNo=isClassNo).item()))\n",
    "    \n",
    "    data_submission[\"class\"] = pd.Series(label)\n",
    "\n",
    "    return data_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0e799-c85f-4851-ab52-d04922492504",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29acd9-0060-4c3e-a294-1866f27de78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184557e-7b61-4d5c-a6c0-707bd506f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"class\"] = data_train[\"class\"].astype(str)\n",
    "data_train[\"conversation\"] = data_train[\"conversation\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bdff8-44a8-43d9-8491-6ec82bbfc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop_duplicates().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1de65-6f79-4f4d-b644-b8c774c66a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLengthVisualization(data_train,['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd760dab-6aa1-4ee4-9f04-d58823343ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "sns.countplot(data=data_train, x='class').set_xlabel('')\n",
    "plt.title('[Class]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ced7f-6450-4c18-a96c-96056e3c9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"./aiffel_dl_thon_dktc_online_15/clean_corpus_DLthon_NO_NO.txt\"\n",
    "#data_preprocessing = preprocessing(data_train.copy(), features=[\"class\",\"conversation\"], corpus_file= corpus_file, max_samples = 1000000, insert_aug=False, max_aug=None, aug_cycle=1)\n",
    "\n",
    "path_to_chatbot_aug = os.path.join(path_to_dataset, 'train_final_NO_NO.csv')\n",
    "data_preprocessing = pd.read_csv(path_to_chatbot_aug)\n",
    "data_preprocessing.columns = ['idx', 'class','conversation']\n",
    "#data_preprocessing.to_csv(os.path.join(path_to_dataset, 'train_final_NO_1.csv'))\n",
    "\n",
    "data_preprocessing.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec371d-3b74-4632-9120-a8298e17366e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DataLengthVisualization(data_preprocessing[data_preprocessing['class']=='협박 대화'],['conversation'])\n",
    "DataLengthVisualization(data_preprocessing[data_preprocessing['class']=='기타 괴롭힘 대화'],['conversation'])\n",
    "DataLengthVisualization(data_preprocessing[data_preprocessing['class']=='갈취 대화'],['conversation'])\n",
    "DataLengthVisualization(data_preprocessing[data_preprocessing['class']=='직장 내 괴롭힘 대화'],['conversation'])\n",
    "DataLengthVisualization(data_preprocessing[data_preprocessing['class']=='일상대화'],['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049c5e0-7bf2-4432-b9b3-4c93be2711ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "sns.countplot(data=data_preprocessing, x='class').set_xlabel('')\n",
    "plt.title('[Class]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c47a3-32aa-406e-9a54-c0b0738359e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLengthVisualization(data_preprocessing,['conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45eac52-f7de-4fe9-8e69-d1905c094cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9baae-c587-407a-b009-d06cd27a36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "print(data_preprocessing['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d9cdc-ab1d-42e4-9c4d-db6fb58bc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa2809-8461-4d51-8de5-4168533432a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_order = np.array(['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일상대화'])\n",
    "le.fit(desired_order)\n",
    "print(\"정렬 순서:\", le.classes_)\n",
    "data_preprocessing[\"class\"] = le.transform(data_preprocessing[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbeba78-a062-4fe9-bd37-c0c2aabe51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5f73e-6519-47f5-b3bf-58c20b7dd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_order(data_preprocessing, le.classes_,['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일상대화'], \"class\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13b0c3-54ee-4b45-a75b-d200bd8b529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece/ tokenization\n",
    "vocab_size = 2**14-5\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input = corpus_file,\n",
    "    model_prefix=\"spm_DLthon\",\n",
    "    vocab_size=vocab_size,\n",
    "    character_coverage=0.9995,\n",
    "    model_type=\"bpe\",\n",
    "    max_sentence_length = 999999,\n",
    "    bos_id=1,  # <s> (Beginning of Sentence) 설정\n",
    "    eos_id=2,  # </s> (End of Sentence) 설정\n",
    "    pad_id=0,  # Padding ID 설정\n",
    "    unk_id=3,   # Unknown Token ID 설정\n",
    "    user_defined_symbols = '<mask>',\n",
    "    shuffle_input_sentence=True\n",
    ")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"spm_DLthon.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb99ca1-6044-4cb5-a8da-1a2609304fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예: 하이퍼파라미터 설정\n",
    "NUM_LAYERS = 4                   # 인코더/디코더 층 수\n",
    "D_MODEL = 1024                   # 임베딩 및 내부 표현 차원\n",
    "NUM_HEADS = 8                    # 멀티헤드 어텐션에서의 헤드 수\n",
    "UNITS = 1024                     # 피드포워드 신경망의 은닉 차원\n",
    "DROPOUT = 0.1                    # 드롭아웃 비율\n",
    "VOCAB_SIZE = sp.get_piece_size() # 단어 집합 크기(예시)\n",
    "WARMUP_STEPS = 4000\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 512\n",
    "NUM_CLASSES = 5\n",
    "SCHEDULER = 'Warmup_cosine'\n",
    "OPTIMIZER = 'AdamW'\n",
    "save_path = \"C:/Users/USER/Desktop/AIFFEL/work/aiffel_dl_thon_dktc_online_15/model\"\n",
    "random_state = random.randint(0, 1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d74fff-e731-49e0-b91f-29c77cc65e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(data_preprocessing, sp, max_length=MAX_LEN, num_classes=NUM_CLASSES, features = ['class', 'conversation'], random_mask_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26698a-3f8c-4d67-bc9e-5f9c991d2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model1 = train_model(EncoderBase, dataset, vocab_size=VOCAB_SIZE, \n",
    "            num_layers=NUM_LAYERS, units=UNITS, d_model=D_MODEL,\n",
    "            num_heads=NUM_HEADS, dropout=DROPOUT, warmup_steps=WARMUP_STEPS, \n",
    "            batch_size=BATCH_SIZE, epochs=50 ,lr = 0.0001, optimize=OPTIMIZER,\n",
    "            verbose = 1, train_ratio=0.9, scheduler=SCHEDULER,_lambda=0.5, alpha=1.0,\n",
    "            patience=6, save_path=save_path, random_state=random_state, aux_available=True, view_aux=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ead3e1-6b3a-4449-8e22-6eda2078e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model1, data_test[\"conversation\"][4],tokenizer=sp, encoder=le, max_len=MAX_LEN, device=device, isClassNo=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5322a-2e25-4458-9cf7-db5ab20b8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_submission = data_inference(model1, data_test, tokenizer=sp, encoder=le, max_len=MAX_LEN, device=device, isClassNo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c748c-7821-4691-90a8-b794502c34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6a99d-51dd-45ea-98f9-944f07c69792",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_submission.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68947867-71e6-489d-88c5-2aadde668a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_submission[[\"idx\",\"class\"]].to_csv(os.path.join(path_to_dataset, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797f1f8-1807-4763-b163-3dcf04c75e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_submission.to_csv(os.path.join(path_to_dataset, 'check.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9156e03-b2a0-4b70-85f3-37ea36c1fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "sns.countplot(data=data_test_submission, x='class').set_xlabel('')\n",
    "plt.title('[Class]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f1959-8700-4981-8384-24b8166bf7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
